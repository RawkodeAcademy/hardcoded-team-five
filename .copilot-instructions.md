# Copilot Instructions for Hardcoded Workshop Team-Five

## Project Overview
This is a microservices text analysis system for the Hardcoded workshop. We're building 11 individual services + 1 aggregator service using multiple programming languages.

## Architecture Requirements
- **Multi-language constraint**: Must use 4+ programming languages
- **Observability constraint**: All services must have `/metrics` endpoint with request counter
- **Testing constraint**: Fuzz testing for critical parsers (Normalizer, Tokenizer)

## Service Contracts
All services follow this pattern:
- `POST /op` - Main logic endpoint
- `GET /healthz` - Health check returning `{ "ok": true }`
- `GET /metrics` - Metrics endpoint with request counter

### Request/Response Format
**Request**: `{ "text": "string", "deps": { "normalized": "string", "transliterated": "string", "tokens": ["array"] } }`
**Response**: `{ "key": "service_name", "value": any, "cache_hit": boolean }`

## Services to Build

### Language Distribution
1. **Go** (3 services): Normalizer, CharCount, Aggregator
2. **Python** (3 services): Transliterator, Bigrams, Entropy  
3. **Node.js/TypeScript** (2 services): Slugger, Palindrome
4. **Rust** (2 services): Tokenizer, Hasher
5. **Java Spring Boot** (1 service): UniqueWords
6. **C# .NET** (1 service): UniqueChars

### Service Details

#### 1. Normalizer (Go)
- NFKC normalize, lowercase, collapse whitespace, strip diacritics
- Returns: `{ "key": "normalized", "value": "string|null", "cache_hit": false }`

#### 2. Transliterator (Python)
- ASCII transliteration from normalized text
- Uses: `deps.normalized`
- Returns: `{ "key": "transliterated", "value": "string|null", "cache_hit": false }`

#### 3. Slugger (Node.js)
- Lowercase [a-z0-9], join with '-', max 64 chars
- Uses: `deps.transliterated`
- Returns: `{ "key": "slug", "value": "string|null", "cache_hit": false }`

#### 4. Tokenizer (Rust)
- Split normalized text on non-alphanumerics, count tokens
- Logic: `normalized.split(/[^a-z0-9]+/).filter(Boolean).length`
- Returns: `{ "key": "tokens", "value": number, "cache_hit": false }`

#### 5. UniqueWords (Java)
- Count distinct tokens from normalized text
- Logic: `new Set(tokensFromNormalized(normalized)).size`
- Returns: `{ "key": "unique_words", "value": number, "cache_hit": false }`

#### 6. Bigrams (Python)
- Count distinct adjacent word pairs from tokens
- Returns: `{ "key": "bigram_count", "value": number, "cache_hit": false }`

#### 7. CharCount (Go)
- Count characters in normalized text
- Logic: `Array.from(normalized).length`
- Returns: `{ "key": "char_count", "value": number, "cache_hit": false }`

#### 8. UniqueChars (C#)
- Count distinct characters in normalized text
- Logic: `new Set(Array.from(normalized)).size`
- Returns: `{ "key": "unique_chars", "value": number, "cache_hit": false }`

#### 9. Hasher (Rust)
- XXHash64 of normalized text, 16-char lowercase hex
- Returns: `{ "key": "hash64", "value": "string", "cache_hit": false }`

#### 10. Entropy (Python)
- Shannon entropy over code points, 3 decimal places
- Returns: `{ "key": "entropy", "value": number, "cache_hit": false }`

#### 11. Palindrome (Node.js)
- Check if alphanumeric normalized text equals its reverse
- Returns: `{ "key": "palindrome", "value": boolean, "cache_hit": false }`

#### 12. Aggregator (Go)
- Calls all services and combines results
- Endpoint: `POST /analyze`
- Returns all fields + `degraded: boolean`
- Environment variables for service URLs: `NORMALIZER_URL`, `TRANSLITERATOR_URL`, etc.

## Implementation Guidelines

### Error Handling
- Tolerate missing/invalid input
- Return reasonable defaults over 500 errors
- Set `degraded: true` if dependencies fail
- Prefer graceful fallbacks

### Metrics Implementation
- Track request count per endpoint
- Include response time if possible
- Use appropriate metrics library for each language
- Prometheus format preferred

### Fuzz Testing
- Create fuzz tests for Normalizer (Unicode edge cases)
- Create fuzz tests for Tokenizer (regex parsing edge cases)
- Use appropriate fuzzing tools for each language

### Environment Configuration
- Each service reads `PORT` environment variable
- Aggregator reads service URLs from environment
- Default ports: 8001-8012 for services, 8000 for aggregator

## Development Priorities
1. Start with simplest services (CharCount, Palindrome)
2. Build dependency chain: Normalizer → Transliterator → Slugger
3. Implement metrics endpoints
4. Add fuzz testing
5. Create Docker deployment

## Common Patterns
When generating code, always include:
- Proper error handling and input validation
- Request counting in metrics
- Health check endpoint
- Environment variable configuration
- Dockerfile for each service
- Clear logging for debugging

## Dependencies Flow
```
normalize → transliterate → slug
normalize → tokens → unique_words
normalize → tokens → bigrams
normalize → char_count
normalize → unique_chars
normalize → hash64
normalize → entropy
normalize → palindrome
```